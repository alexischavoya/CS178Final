{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"> Mauricio Lomeli 23329506</div>\n",
    "<div style=\"text-align: right\"> Alexis Chavoya\t55614050</div>\n",
    "<div style=\"text-align: right\"> erichyf</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS 178: Machine Learning & Data Mining: Fall 2020\n",
    "# model_name_1\n",
    "#### Due Date: , December 11, 2020\n",
    "\n",
    "<p>For each model, a paragraph or two describing: what features you gave it (raw inputs, selected inputs, non-linear feature expansions, etc.); how was it trained (learning algorithm and software source); and key hyperparameter settings (plus your approach to choosing those settings)</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mltools as ml\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the features we gave (raw inputs, selected inputs, nonlinear feature expansions, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X = np.genfromtxt(\"data/X_train.txt\",delimiter=',')\n",
    "Y = np.genfromtxt(\"data/Y_train.txt\",delimiter=',')\n",
    "X,Y = ml.shuffleData(X,Y)\n",
    "\n",
    "first_five = X[:, :5].T\n",
    "for i, feature in enumerate(first_five):\n",
    "    print(\"feature \", i + 1)\n",
    "    print(\"\\tminimum =\", np.min(feature))\n",
    "    print(\"\\tmaximum =\", np.max(feature))\n",
    "    print(\"\\tmean =\", np.mean(feature))\n",
    "    print(\"\\tvariance =\", np.var(feature))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how was it trained (learning algorithm and software source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Xtr, Xva, Ytr, Yva = ml.splitData(X, Y, 0.75)\n",
    "\n",
    "learner = ml.dtree.treeClassify(Xtr, Ytr, maxDepth=50)\n",
    "print(\"Training Error:\", learner.err(Xtr, Ytr))\n",
    "print(\"Validation Error:\", learner.err(Xva, Yva))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Settings\n",
    "key hyperparameter settings (plus your approach to choosing those settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Max Depth Parameter\n",
    "<span style=\"color:blue\">Ex. Now try varying the maxDepth parameter, which forces the tree learning algorithm to stop after at most that many levels. Test maxDepth values in the range 0, 1, 2, ..., 15 , and plot the training and validation error rates versus maxDepth. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tre = []\n",
    "vae = []\n",
    "\n",
    "for depth in range(16):\n",
    "    learner = ml.dtree.treeClassify(Xtr, Ytr, maxDepth=depth, minParent=1, minLeaf=2)\n",
    "    tre.append(learner.err(Xtr, Ytr))\n",
    "    vae.append(learner.err(Xva, Yva))\n",
    "\n",
    "\n",
    "print(\"Optimal Depth:\", np.argmin(vae))\n",
    "print(\"Error:\", np.min(vae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(range(16), tre)\n",
    "plt.plot(range(16), vae)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Minimum Parent parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Minimum Leaf Parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. For the best decision tree model trained in the previous parts, use the roc function to plot an ROC curve summarizing your classifier performance on the training points, and another ROC curve summarizing your performance on the validation points. Then using the auc function, compute and report the AUC scores for the training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# this code won't work but its just a template \n",
    "learner = ml.dtree.treeClassify(Xtr, Ytr, maxDepth=depth)\n",
    "ftr, ttr, _ = learner.roc(Xtr, Ytr)\n",
    "fva, tva, _ = learner.roc(Xva, Yva)\n",
    "\n",
    "print(\"Training AUC:\", learner.auc(Xtr, Ytr))\n",
    "print(\"Validation AUC:\", learner.auc(Xva, Yva))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(ftr, ttr)\n",
    "plt.plot(fva, tva)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
